---
layout: page
title: "Glosasry"
---

# Glosasry

Here are some of the basic terms and concepts.
We will get into to more detail later on so don't spend too much time here.
Having said so, it is worth a skim.

- Random variable: a variable whose possible values are numerical outcomes (or the representations of) of a random experiment
    - often denoted as X
    - is split up into 'Discrete' RV & 'Continuous' RV
- Discrete vs Continuous: are categories of numeric variables
    - Discrete: countable data
    - Continuous: measured data (i.e. any value that falls between a range)
    - think of it as a question of units - you can't have 1.23 people, but the height of a person can be 180.1341...cm
    - some find it helpful to think counted or measured, but I simply think - if not discrete, then continuous
        - you'll get more used to it as you go on
- Expected value: the arithmetic mean of the possible values a random variable can take, weighted by the probability of those outcomes
    - also known as expectation or mean
    - you will understand better when you see the equation
- Indicator function: this is a function that takes the value one if its argument is true, and the value zero if its argument is false
- Discrete Distributions:
    - Bernoulli: two possible outcomes for a single trial
    - Binomial: two possible outcomes
    - Geometric: number of trials needed to get first success
    - Negative Binomial: number of failures before x-th success
    - Multinomial: a binomial with more than two possible outcomes
    - Poisson: just remember frequency for now
- Continuous Distributions:
    - Exponential: waiting time between different events
    - Gamma: total waiting time
    - Uniform: all equally likely
    - Beta: probability of probabilities
    - Normal: the Gaussian distribution
    - t: normal distribution with unknowns


Note that different sources will use different terms and greeks to explain these concepts, 
but I am trying to be as consistent as possible throughout this entire book.